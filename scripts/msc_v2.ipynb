{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Merge daily dataframes to one single dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-13T22:51:23.707160Z",
     "end_time": "2023-03-13T22:51:24.375160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 dates included (2022-10-31 to 2023-03-12).\n"
     ]
    }
   ],
   "source": [
    "# Define the date range\n",
    "start_date = '2022-10-31'\n",
    "end_date = '2023-03-12'\n",
    "\n",
    "# Create a list of dates in YYYY-MM-DD format\n",
    "dates = pd.date_range(start_date, end_date).strftime(\"%Y-%m-%d\").tolist()\n",
    "print(f\"{len(dates)} dates included ({start_date} to {end_date}).\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-13T22:51:24.375160Z",
     "end_time": "2023-03-13T22:51:24.485775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2022-11-10 missing!\n",
      "Data for 2022-11-23 missing!\n",
      "Data for 2022-11-24 missing!\n",
      "Data for 2022-11-25 missing!\n",
      "Data for 2022-11-26 missing!\n",
      "Data for 2022-11-27 missing!\n",
      "Data for 2022-11-28 missing!\n",
      "Data for 2022-12-14 missing!\n",
      "Data for 2023-02-07 missing!\n",
      "Data for 2023-03-09 missing!\n"
     ]
    }
   ],
   "source": [
    "# Loop through the dates and read the pickle files. Each pickle file contains a list of dictionaries, each dictionary contains the details of one route.\n",
    "processed_list = []\n",
    "missing = 0\n",
    "for date in dates:\n",
    "    # Read the pickle for each date. It contains a list of dictionaries, each dictionary contains the details of one harbor combination.\n",
    "    try:\n",
    "        with open(f'../pickles/msc_daily_v2/connections_{date}.pickle', 'rb') as f:\n",
    "            results_list = pickle.load(f)\n",
    "    except:\n",
    "        missing += 1\n",
    "        print(f\"Data for {date} missing!\")\n",
    "\n",
    "    # Unpack the list of dictionaries. Each dictionary contains the details of one route. The \"Key\" key contains another dictionary with the route details. Flatten it into the main dictionary. Put the new items at the beginning of the dictionary.\n",
    "    for result in results_list:\n",
    "        # Flatten the information in the \"Keys\" dictionary.\n",
    "        result = (result['Key'] | result)\n",
    "        result.pop('Key')\n",
    "\n",
    "        # Create a dictionary with the details that's identical for all routes in the list.\n",
    "        non_route_specific_results = result.copy()\n",
    "        non_route_specific_results.pop(\"Routes\")\n",
    "\n",
    "        # Loop through the list of routes, create a new dictionary for each route and append it to the list.\n",
    "        routes_details_list = result['Routes']\n",
    "        for route in routes_details_list:\n",
    "            # Merge the non-route-specific details with the route-specific details.\n",
    "            new_dict = non_route_specific_results.copy()\n",
    "            new_dict[\"NumberOfLegs\"] = len(route[\"RouteScheduleLegDetails\"])\n",
    "            new_dict = (new_dict | route)\n",
    "            new_dict[\"ScrapingDate\"] = date\n",
    "            processed_list.append(new_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-13T22:51:24.422070Z",
     "end_time": "2023-03-13T22:51:30.786759Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of routes: 126918, spread over 123 days.\n",
      "54296 identical rows dropped, 42.78% of the total number of rows.\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe from the list of dictionaries\n",
    "df = pd.DataFrame(processed_list)\n",
    "\n",
    "# Move the second last and third last columns to the beginning of the dataframe\n",
    "cols = df.columns.tolist()\n",
    "cols = [cols[-1]] + cols[-3:-1] + cols[:-3]\n",
    "df = df[cols]\n",
    "\n",
    "print(f\"Total number of routes: {len(df)}, spread over {len(dates)-missing} days.\")\n",
    "\n",
    "# Assign the optimal data types to the columns.\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# Drop rows which are identical, except for the ScrapingDate column.\n",
    "rows_before = len(df)\n",
    "columns_to_check = [col for col in df.columns.to_list() if col not in [\"ScrapingDate\", \"CutOffs\", \"RouteScheduleLegDetails\"]]\n",
    "\n",
    "df = df.drop_duplicates(subset=columns_to_check, keep='first')\n",
    "\n",
    "# Print the number of identical rows dropped.\n",
    "print(f\"{rows_before - len(df)} identical rows dropped, {1-(len(df)/rows_before):.2%} of the total number of rows.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-13T22:51:30.802402Z",
     "end_time": "2023-03-13T22:51:32.965084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Save the dataframe to a pickle file.\n",
    "filename = \"msc_v2_connections_combined\"\n",
    "df.to_pickle(f\"../pickles/{filename}.pickle\")\n",
    "df.to_csv(f\"../data/{filename}.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-03-13T22:51:32.965084Z",
     "end_time": "2023-03-13T22:51:43.588000Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
