{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Web scraping of RouteScanner\n",
    "\n",
    "A first notebook to web scrape Route Scanner."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Quick and dirty hack because my PyCharm interpreter is not playing nice\n",
    "ewouts_broken_PyCharm = True\n",
    "if ewouts_broken_PyCharm:\n",
    "    import sys\n",
    "    sys.path.append('C:\\\\Users\\\\Ewout\\\\Documents\\\\python_venv\\\\Py310')\n",
    "    sys.path.append('C:\\\\Users\\\\Ewout\\\\Documents\\\\python_venv\\\\Py310\\\\lib\\\\site-packages')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Direct connections\n",
    "\n",
    "In this part a dataframe is created with all direct connections between sea terminals in South America and The Netherlands/Belgium.\n",
    "\n",
    "Data: https://www.routescanner.com/services/direct-connections\n",
    "\n",
    "**TODO:** Make sure more than 20 routes per webpage can be scraped (the results on page 2 and onwards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Webscraping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BR', 'NL'), ('BR', 'BE'), ('CO', 'NL'), ('CO', 'BE'), ('VE', 'NL'), ('VE', 'BE'), ('SR', 'NL'), ('SR', 'BE'), ('CW', 'NL'), ('CW', 'BE'), ('GY', 'NL'), ('GY', 'BE'), ('GF', 'NL'), ('GF', 'BE'), ('UY', 'NL'), ('UY', 'BE'), ('AR', 'NL'), ('AR', 'BE'), ('CL', 'NL'), ('CL', 'BE'), ('PE', 'NL'), ('PE', 'BE'), ('EC', 'NL'), ('EC', 'BE')]\n"
     ]
    }
   ],
   "source": [
    "# Define origin and destination countries\n",
    "origin = [\"BR\", \"CO\", \"VE\", \"SR\", \"CW\", \"GY\", \"GF\", \"UY\", \"AR\", \"CL\", \"PE\", \"EC\"]\n",
    "destination = [\"NL\", \"BE\"]\n",
    "\n",
    "# Make list with all combinations\n",
    "od_list = list(itertools.product(origin, destination))\n",
    "print(od_list)\n",
    "\n",
    "# Create URL list to scrape\n",
    "urls = [f\"https://www.routescanner.com/services/direct-connections/results?destinationCountries={o}&originCountries={d}&search=advanced\" for o, d in od_list]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_webpage(url, headless=False):\n",
    "    # Instantiate options\n",
    "    opts = Options()\n",
    "    opts.binary_location = \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "    if headless:\n",
    "        opts.headless = True\n",
    "\n",
    "    # Set the location of the webdriver\n",
    "    chrome_driver = os.getcwd() + \"/drivers/chromedriver.exe\"\n",
    "\n",
    "    # Instantiate a webdriver\n",
    "    driver = webdriver.Chrome(options=opts, executable_path=chrome_driver)\n",
    "\n",
    "    # Load the HTML page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Accept cookies\n",
    "    driver.implicitly_wait(2)\n",
    "    driver.find_element(By.CLASS_NAME,\"acceptButton__P2szu\").click()\n",
    "\n",
    "    # Wait untill route data is loaded\n",
    "    try:\n",
    "        elem = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.CLASS_NAME, 'card__hoI9D')))\n",
    "    except:\n",
    "        print('No connections found on this route')\n",
    "\n",
    "    # Parse processed webpage with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.close()\n",
    "    return soup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ewout\\AppData\\Local\\Temp\\ipykernel_26040\\438299618.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(options=opts, executable_path=chrome_driver)\n"
     ]
    }
   ],
   "source": [
    "# A test run with a single webpage\n",
    "soup1 = get_webpage(urls[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Extract data from HTML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Returns route data for list of routes\n",
    "def get_route_data(soup):\n",
    "    # Create a list with all route cards\n",
    "    routes = soup.find_all(\"div\", class_=\"card__hoI9D\")\n",
    "\n",
    "    # Create empty list for the route data\n",
    "    route_data = []\n",
    "\n",
    "    # For each route, find the company, origin, destination, service code, duration and if known frequency\n",
    "    for route in routes:\n",
    "        c = route.find(\"h6\").text\n",
    "        o, d = route.find(\"small\", class_=\"locodes__tdIbs\").text.split(' - ')\n",
    "        s = route.find(\"p\", class_=\"serviceCode__igooW\").text\n",
    "        t = route.find(\"div\", class_=\"times__xgwdu\").find_all(\"div\")\n",
    "        t = [i.text for i in t]\n",
    "        route_data.append([c, o, d, s, *t])\n",
    "    return route_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "       Company Origin Destination                   Service Code Duration  \\\n0          MSC  NLRTM       BRRIG         NWC TO SAEC - STRING I  43 days   \n1          MSC  NLRTM       BRITJ         NWC TO SAEC - STRING I  35 days   \n2          MSC  NLRTM       BRRIO         NWC TO SAEC - STRING I  30 days   \n3  Hapag-Lloyd  NLRTM       BRITJ  ECX-EUROPE EAST COAST EXPRESS  34 days   \n4          MSC  NLRTM       BRRIO         NWC TO SAEC - STRING I  31 days   \n\n          Frequency  \n0  2 times per week  \n1   1 time per week  \n2              None  \n3              None  \n4   1 time per week  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Company</th>\n      <th>Origin</th>\n      <th>Destination</th>\n      <th>Service Code</th>\n      <th>Duration</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRRIG</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>43 days</td>\n      <td>2 times per week</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRITJ</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>35 days</td>\n      <td>1 time per week</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRRIO</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>30 days</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hapag-Lloyd</td>\n      <td>NLRTM</td>\n      <td>BRITJ</td>\n      <td>ECX-EUROPE EAST COAST EXPRESS</td>\n      <td>34 days</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRRIO</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>31 days</td>\n      <td>1 time per week</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get route data sample\n",
    "data1 = get_route_data(soup1)\n",
    "\n",
    "# Define dict keys / dataframe columns\n",
    "columns = [\"Company\", \"Origin\", \"Destination\", \"Service Code\", \"Duration\", \"Frequency\"]\n",
    "\n",
    "# Create dataframe from the data sample\n",
    "df1 = pd.DataFrame(data1, columns=columns)\n",
    "df1.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run on all routes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "       Company Origin Destination                   Service Code Duration  \\\n0          MSC  NLRTM       BRRIG         NWC TO SAEC - STRING I  43 days   \n1          MSC  NLRTM       BRITJ         NWC TO SAEC - STRING I  35 days   \n2          MSC  NLRTM       BRRIO         NWC TO SAEC - STRING I  30 days   \n3  Hapag-Lloyd  NLRTM       BRITJ  ECX-EUROPE EAST COAST EXPRESS  34 days   \n4          MSC  NLRTM       BRRIO         NWC TO SAEC - STRING I  31 days   \n\n          Frequency  \n0  2 times per week  \n1   1 time per week  \n2              None  \n3              None  \n4   1 time per week  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Company</th>\n      <th>Origin</th>\n      <th>Destination</th>\n      <th>Service Code</th>\n      <th>Duration</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRRIG</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>43 days</td>\n      <td>2 times per week</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRITJ</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>35 days</td>\n      <td>1 time per week</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRRIO</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>30 days</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hapag-Lloyd</td>\n      <td>NLRTM</td>\n      <td>BRITJ</td>\n      <td>ECX-EUROPE EAST COAST EXPRESS</td>\n      <td>34 days</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MSC</td>\n      <td>NLRTM</td>\n      <td>BRRIO</td>\n      <td>NWC TO SAEC - STRING I</td>\n      <td>31 days</td>\n      <td>1 time per week</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data can be loaded from a pickle, so the script doesn't need to re-run every time.\n",
    "# Set use_pickle1 to False if you want to scrape all the data again.\n",
    "use_pickle1 = True\n",
    "\n",
    "if use_pickle1:\n",
    "    route_df = pd.read_pickle(\"pickles/routes_between_ports.pickle\")\n",
    "\n",
    "else:\n",
    "    # Empty list for data and len\n",
    "    route_data = []\n",
    "    combinations = len(urls)\n",
    "\n",
    "    for n, url in enumerate(urls):\n",
    "        # Scrape the webpage\n",
    "        soup = get_webpage(url, headless=True)\n",
    "        # Get all the route data, process them and add\n",
    "        new_route_data = get_route_data(soup)\n",
    "        route_data = route_data + new_route_data\n",
    "        # Nice print message\n",
    "        print(f\"Done with {n}/{combinations} webpages, {len(new_route_data)} new routes found (total {len(route_data)})\")\n",
    "\n",
    "    # Create a dataframe from all the route data\n",
    "    route_df = pd.DataFrame(route_data, columns=columns)\n",
    "route_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Save as Pickle and CSV\n",
    "route_df.to_pickle(\"pickles/routes_between_ports.pickle\")\n",
    "route_df.to_csv(\"data/routes_between_ports.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
